{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from torchvision.models.inception import Inception3, BasicConv2d\n",
    "import time\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "n_epochs = 30\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 100\n",
    "width=128\n",
    "\n",
    "torch.backends.cudnn.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing sets\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./MNIST/', train=True, download=False, # Change download to True if not already downloaded\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./MNIST/', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, lossFunc, optimizer):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = lossFunc(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(network, epoch):\n",
    "    network.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = network(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "    print('Epoch {}, Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "          epoch, correct, len(test_loader.dataset),\n",
    "          100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetDense, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "class NetConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetConv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "networkDense = NetDense()\n",
    "optimizerDense = optim.SGD(networkDense.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum, weight_decay = learning_rate)\n",
    "\n",
    "networkConv1 = NetConv()\n",
    "optimizerConv1 = optim.SGD(networkConv1.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum, weight_decay = learning_rate)\n",
    "\n",
    "networkConv2 = NetConv()\n",
    "optimizerConv2 = optim.SGD(networkConv2.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum, weight_decay = learning_rate)\n",
    "\n",
    "\n",
    "lossFunc = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test set: Accuracy: 1358/10000 (13%)\n",
      "Epoch 1, Test set: Accuracy: 9210/10000 (92%)\n",
      "Epoch 2, Test set: Accuracy: 9388/10000 (93%)\n",
      "Epoch 3, Test set: Accuracy: 9455/10000 (94%)\n",
      "Epoch 4, Test set: Accuracy: 9486/10000 (94%)\n",
      "Epoch 5, Test set: Accuracy: 9493/10000 (94%)\n",
      "Epoch 6, Test set: Accuracy: 9531/10000 (95%)\n",
      "Epoch 7, Test set: Accuracy: 9544/10000 (95%)\n",
      "Epoch 8, Test set: Accuracy: 9545/10000 (95%)\n",
      "Epoch 9, Test set: Accuracy: 9542/10000 (95%)\n",
      "Epoch 10, Test set: Accuracy: 9560/10000 (95%)\n",
      "Epoch 11, Test set: Accuracy: 9563/10000 (95%)\n",
      "Epoch 12, Test set: Accuracy: 9571/10000 (95%)\n",
      "Epoch 13, Test set: Accuracy: 9586/10000 (95%)\n",
      "Epoch 14, Test set: Accuracy: 9586/10000 (95%)\n",
      "Epoch 15, Test set: Accuracy: 9585/10000 (95%)\n",
      "Epoch 16, Test set: Accuracy: 9589/10000 (95%)\n",
      "Epoch 17, Test set: Accuracy: 9587/10000 (95%)\n",
      "Epoch 18, Test set: Accuracy: 9590/10000 (95%)\n",
      "Epoch 19, Test set: Accuracy: 9600/10000 (96%)\n",
      "Epoch 20, Test set: Accuracy: 9597/10000 (95%)\n",
      "Epoch 21, Test set: Accuracy: 9590/10000 (95%)\n",
      "Epoch 22, Test set: Accuracy: 9615/10000 (96%)\n",
      "Epoch 23, Test set: Accuracy: 9597/10000 (95%)\n",
      "Epoch 24, Test set: Accuracy: 9611/10000 (96%)\n",
      "Epoch 25, Test set: Accuracy: 9596/10000 (95%)\n",
      "Epoch 26, Test set: Accuracy: 9602/10000 (96%)\n",
      "Epoch 27, Test set: Accuracy: 9611/10000 (96%)\n",
      "Epoch 28, Test set: Accuracy: 9591/10000 (95%)\n",
      "Epoch 29, Test set: Accuracy: 9587/10000 (95%)\n",
      "Epoch 30, Test set: Accuracy: 9603/10000 (96%)\n"
     ]
    }
   ],
   "source": [
    "test(networkDense, 0)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(networkDense, lossFunc, optimizerDense)\n",
    "    test(networkDense, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test set: Accuracy: 1000/10000 (10%)\n",
      "Epoch 1, Test set: Accuracy: 9370/10000 (93%)\n",
      "Epoch 2, Test set: Accuracy: 9589/10000 (95%)\n",
      "Epoch 3, Test set: Accuracy: 9652/10000 (96%)\n",
      "Epoch 4, Test set: Accuracy: 9679/10000 (96%)\n",
      "Epoch 5, Test set: Accuracy: 9733/10000 (97%)\n",
      "Epoch 6, Test set: Accuracy: 9741/10000 (97%)\n",
      "Epoch 7, Test set: Accuracy: 9763/10000 (97%)\n",
      "Epoch 8, Test set: Accuracy: 9771/10000 (97%)\n",
      "Epoch 9, Test set: Accuracy: 9774/10000 (97%)\n",
      "Epoch 10, Test set: Accuracy: 9765/10000 (97%)\n",
      "Epoch 11, Test set: Accuracy: 9774/10000 (97%)\n",
      "Epoch 12, Test set: Accuracy: 9776/10000 (97%)\n",
      "Epoch 13, Test set: Accuracy: 9787/10000 (97%)\n",
      "Epoch 14, Test set: Accuracy: 9779/10000 (97%)\n",
      "Epoch 15, Test set: Accuracy: 9770/10000 (97%)\n",
      "Epoch 16, Test set: Accuracy: 9802/10000 (98%)\n",
      "Epoch 17, Test set: Accuracy: 9794/10000 (97%)\n",
      "Epoch 18, Test set: Accuracy: 9796/10000 (97%)\n",
      "Epoch 19, Test set: Accuracy: 9795/10000 (97%)\n",
      "Epoch 20, Test set: Accuracy: 9798/10000 (97%)\n",
      "Epoch 21, Test set: Accuracy: 9794/10000 (97%)\n",
      "Epoch 22, Test set: Accuracy: 9809/10000 (98%)\n",
      "Epoch 23, Test set: Accuracy: 9820/10000 (98%)\n",
      "Epoch 24, Test set: Accuracy: 9799/10000 (97%)\n",
      "Epoch 25, Test set: Accuracy: 9798/10000 (97%)\n",
      "Epoch 26, Test set: Accuracy: 9814/10000 (98%)\n",
      "Epoch 27, Test set: Accuracy: 9787/10000 (97%)\n",
      "Epoch 28, Test set: Accuracy: 9808/10000 (98%)\n",
      "Epoch 29, Test set: Accuracy: 9803/10000 (98%)\n",
      "Epoch 30, Test set: Accuracy: 9807/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "test(networkConv1, 0)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(networkConv1, lossFunc, optimizerConv1)\n",
    "    test(networkConv1, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test set: Accuracy: 998/10000 (9%)\n",
      "Epoch 1, Test set: Accuracy: 9387/10000 (93%)\n",
      "Epoch 2, Test set: Accuracy: 9601/10000 (96%)\n",
      "Epoch 3, Test set: Accuracy: 9677/10000 (96%)\n",
      "Epoch 4, Test set: Accuracy: 9703/10000 (97%)\n",
      "Epoch 5, Test set: Accuracy: 9707/10000 (97%)\n",
      "Epoch 6, Test set: Accuracy: 9742/10000 (97%)\n",
      "Epoch 7, Test set: Accuracy: 9746/10000 (97%)\n",
      "Epoch 8, Test set: Accuracy: 9764/10000 (97%)\n",
      "Epoch 9, Test set: Accuracy: 9766/10000 (97%)\n",
      "Epoch 10, Test set: Accuracy: 9750/10000 (97%)\n",
      "Epoch 11, Test set: Accuracy: 9752/10000 (97%)\n",
      "Epoch 12, Test set: Accuracy: 9760/10000 (97%)\n",
      "Epoch 13, Test set: Accuracy: 9792/10000 (97%)\n",
      "Epoch 14, Test set: Accuracy: 9779/10000 (97%)\n",
      "Epoch 15, Test set: Accuracy: 9784/10000 (97%)\n",
      "Epoch 16, Test set: Accuracy: 9778/10000 (97%)\n",
      "Epoch 17, Test set: Accuracy: 9776/10000 (97%)\n",
      "Epoch 18, Test set: Accuracy: 9778/10000 (97%)\n",
      "Epoch 19, Test set: Accuracy: 9787/10000 (97%)\n",
      "Epoch 20, Test set: Accuracy: 9793/10000 (97%)\n",
      "Epoch 21, Test set: Accuracy: 9792/10000 (97%)\n",
      "Epoch 22, Test set: Accuracy: 9788/10000 (97%)\n",
      "Epoch 23, Test set: Accuracy: 9803/10000 (98%)\n",
      "Epoch 24, Test set: Accuracy: 9808/10000 (98%)\n",
      "Epoch 25, Test set: Accuracy: 9787/10000 (97%)\n",
      "Epoch 26, Test set: Accuracy: 9798/10000 (97%)\n",
      "Epoch 27, Test set: Accuracy: 9801/10000 (98%)\n",
      "Epoch 28, Test set: Accuracy: 9786/10000 (97%)\n",
      "Epoch 29, Test set: Accuracy: 9804/10000 (98%)\n",
      "Epoch 30, Test set: Accuracy: 9804/10000 (98%)\n"
     ]
    }
   ],
   "source": [
    "test(networkConv2, 0)\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    train(networkConv2, lossFunc, optimizerConv2)\n",
    "    test(networkConv2, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing with batch size 1\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./MNIST/', train=True, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=1, shuffle=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./MNIST/', train=False, download=False,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This calculates the whichOut row of the Jacobians for a given net and set of data\n",
    "def computeAverage(net, data_loader, whichOut):\n",
    "    numSamples = len(data_loader)\n",
    "    grads = np.zeros((numSamples, 784 + 1))\n",
    "    outputs = np.zeros(numSamples)\n",
    "    \n",
    "    outConsidering = torch.LongTensor([whichOut])\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data.requires_grad_()\n",
    "        net.zero_grad()\n",
    "        \n",
    "        output = net.forward(data)\n",
    "    \n",
    "        g = torch.autograd.grad(output[:, whichOut], data)[0].data\n",
    "        g = g.view(-1, 28*28)\n",
    "        \n",
    "        \n",
    "        outputs[batch_idx] = output[:, whichOut].detach().numpy()\n",
    "        grads[batch_idx, :] = np.array(g.tolist()[0] + [output[:, whichOut] - torch.dot(torch.squeeze(data.view(-1, 28*28)), torch.squeeze(g))])\n",
    "    \n",
    "    return outputs, grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the save locations to a preferred location\n",
    "# Warning - large file sizes\n",
    "for i in range(10):\n",
    "    _, linearRegionsDense = computeAverage(networkDense, train_loader, i)\n",
    "    np.save('/s/red/b/nobackup/data/bsattelb/linearRegions/denseMNISTTrainRegion' + str(i) + '.npy', linearRegionsDense)\n",
    "    del linearRegionsDense\n",
    "    _, linearRegionsConv1 = computeAverage(networkConv1, train_loader, i)\n",
    "    np.save('/s/red/b/nobackup/data/bsattelb/linearRegions/conv1MNISTTrainRegion' + str(i) + '.npy', linearRegionsConv1)\n",
    "    del linearRegionsConv1\n",
    "    _, linearRegionsConv2 = computeAverage(networkConv2, train_loader, i)\n",
    "    np.save('/s/red/b/nobackup/data/bsattelb/linearRegions/conv2MNISTTrainRegion' + str(i) + '.npy', linearRegionsConv2)\n",
    "    del linearRegionsConv2\n",
    "    \n",
    "    _, linearRegionsDense = computeAverage(networkDense, test_loader, i)\n",
    "    np.save('/s/red/b/nobackup/data/bsattelb/linearRegions/denseMNISTTestRegion' + str(i) + '.npy', linearRegionsDense)\n",
    "    del linearRegionsDense\n",
    "    _, linearRegionsConv1 = computeAverage(networkConv1, test_loader, i)\n",
    "    np.save('/s/red/b/nobackup/data/bsattelb/linearRegions/conv1MNISTTestRegion' + str(i) + '.npy', linearRegionsConv1)\n",
    "    del linearRegionsConv1\n",
    "    _, linearRegionsConv2 = computeAverage(networkConv2, test_loader, i)\n",
    "    np.save('/s/red/b/nobackup/data/bsattelb/linearRegions/conv2MNISTTestRegion' + str(i) + '.npy', linearRegionsConv2)\n",
    "    del linearRegionsConv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
